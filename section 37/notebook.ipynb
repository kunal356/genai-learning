{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n",
      "c:\\Users\\ashwi\\kunal_programming\\UdemySelfLearning\\krish-genai-course\\venv-genai\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEndpoint(repo_id='mistralai/Mistral-7B-Instruct-v0.3', temperature=0.7, model_kwargs={'max_length': 150}, model='mistralai/Mistral-7B-Instruct-v0.3', client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.3', timeout=120)>, async_client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.3', timeout=120)>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "llm = HuggingFaceEndpoint(repo_id=repo_id, max_length=150, temperature=0.7)\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'?\\n\\nMachine learning is a subset of artificial intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. Machine learning focuses on the development of computer programs that can access data and use it to learn for themselves.\\n\\nThe process of learning begins with observations or data, such as examples, direct experience, or instruction, in order to look for patterns in data and make better decisions in the future based on the examples that we provide. The primary aim is to allow the computers to learn automatically without human intervention or assistance and adjust actions accordingly.\\n\\nThere are three main categories of machine learning:\\n\\n- Supervised learning: The computer is presented with example inputs and their desired outputs, given by a \"teacher\", and the goal is to learn a general rule that maps inputs to outputs.\\n- Unsupervised learning: Unsupervised learning is used for exploratory data analysis to find hidden patterns or intrinsic structures from input data.\\n- Reinforcement learning: An agent learns to behave in an environment by performing actions and observing the results. The goal is to learn a series of actions that maximize some notion of cumulative reward.\\n\\nWhat is the difference between machine learning and deep learning?\\n\\nMachine learning is a subset of artificial intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. Deep learning is a subset of machine learning that is based on artificial neural networks with representation learning.\\n\\nIn other words, deep learning is a machine learning technique that uses artificial neural networks to learn representations of data with multiple levels of abstraction. It is called deep because the neural networks are deep, consisting of multiple hidden layers.\\n\\nDeep learning is used for tasks such as image and speech recognition, natural language processing, and autonomous vehicles. It is a powerful tool for handling large and complex datasets and is capable of achieving state-of-the-art results in many areas.\\n\\nWhat are some examples of machine learning in action?\\n\\nThere are many examples of machine learning in action, ranging from everyday applications to more advanced and specialized uses. Here are a few examples:\\n\\n1. Recommendation systems: Machine learning is used to power recommendation systems on platforms such as Netflix, Amazon, and Spotify. These systems analyze user behavior and preferences to suggest products, movies, or music that they might enjoy.\\n2. Fraud detection: Machine learning is used in'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is machine learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'?\\n\\nGenerative AI refers to a type of artificial intelligence (AI) that can create new content, such as images, text, music, or even code, based on the patterns it has learned from the data it was trained on. Unlike discriminative models, which are designed to make predictions or classifications based on existing data, generative models are designed to generate new data that resembles the data they were trained on.\\n\\nGenerative AI models use various techniques, such as deep learning, recurrent neural networks (RNNs), and generative adversarial networks (GANs), to learn the underlying patterns and structures in the data they were trained on. Once they have learned these patterns, they can use them to generate new content that is similar in style and quality to the original data.\\n\\nGenerative AI has a wide range of applications, from creating realistic images and videos to generating new music and text. It is also used in fields such as medicine, finance, and marketing to generate new insights and predictions based on large datasets.\\n\\nGenerative AI is a rapidly evolving field, with new techniques and applications being developed all the time. As the technology continues to improve, it is expected to have a significant impact on a wide range of industries and fields in the coming years.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is generative ai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEndpoint(repo_id='google/gemma-2-9b', temperature=0.7, model_kwargs={'max_length': 150}, model='google/gemma-2-9b', client=<InferenceClient(model='google/gemma-2-9b', timeout=120)>, async_client=<InferenceClient(model='google/gemma-2-9b', timeout=120)>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id = \"google/gemma-2-9b\"\n",
    "llm = HuggingFaceEndpoint(repo_id=repo_id, max_length=150, temperature=0.7)\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": " (Request ID: J4weV2wAsUXoJZM0JHFcI)\n\n403 Forbidden: None.\nCannot access content at: https://api-inference.huggingface.co/models/google/gemma-2-9b.\nIf you are trying to create or update content, make sure you have a token with the `write` role.\nThe model google/gemma-2-9b is too large to be loaded automatically (36GB > 10GB). Please use Spaces (https://huggingface.co/spaces) or Inference Endpoints (https://huggingface.co/inference-endpoints).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\ashwi\\kunal_programming\\UdemySelfLearning\\krish-genai-course\\venv-genai\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 304\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\ashwi\\kunal_programming\\UdemySelfLearning\\krish-genai-course\\venv-genai\\lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://api-inference.huggingface.co/models/google/gemma-2-9b",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is rate of interest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ashwi\\kunal_programming\\UdemySelfLearning\\krish-genai-course\\venv-genai\\lib\\site-packages\\langchain_core\\language_models\\llms.py:344\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    340\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    341\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    342\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 344\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    345\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    346\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    347\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    348\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    349\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    350\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    351\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    352\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    353\u001b[0m         )\n\u001b[0;32m    354\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    355\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    356\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ashwi\\kunal_programming\\UdemySelfLearning\\krish-genai-course\\venv-genai\\lib\\site-packages\\langchain_core\\language_models\\llms.py:701\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    693\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    694\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    695\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    698\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    699\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    700\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 701\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_strings, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ashwi\\kunal_programming\\UdemySelfLearning\\krish-genai-course\\venv-genai\\lib\\site-packages\\langchain_core\\language_models\\llms.py:880\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    865\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    866\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    867\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    868\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    878\u001b[0m         )\n\u001b[0;32m    879\u001b[0m     ]\n\u001b[1;32m--> 880\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_helper(\n\u001b[0;32m    881\u001b[0m         prompts, stop, run_managers, \u001b[38;5;28mbool\u001b[39m(new_arg_supported), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    882\u001b[0m     )\n\u001b[0;32m    883\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    884\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\ashwi\\kunal_programming\\UdemySelfLearning\\krish-genai-course\\venv-genai\\lib\\site-packages\\langchain_core\\language_models\\llms.py:738\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[0;32m    737\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 738\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    739\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    740\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32mc:\\Users\\ashwi\\kunal_programming\\UdemySelfLearning\\krish-genai-course\\venv-genai\\lib\\site-packages\\langchain_core\\language_models\\llms.py:725\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    716\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    717\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    721\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    722\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    723\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    724\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 725\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    726\u001b[0m                 prompts,\n\u001b[0;32m    727\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    728\u001b[0m                 \u001b[38;5;66;03m# TODO: support multiple run managers\u001b[39;00m\n\u001b[0;32m    729\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    730\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    731\u001b[0m             )\n\u001b[0;32m    732\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    733\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    734\u001b[0m         )\n\u001b[0;32m    735\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    736\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Users\\ashwi\\kunal_programming\\UdemySelfLearning\\krish-genai-course\\venv-genai\\lib\\site-packages\\langchain_core\\language_models\\llms.py:1429\u001b[0m, in \u001b[0;36mLLM._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1426\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1427\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[0;32m   1428\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 1429\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1430\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m   1431\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1432\u001b[0m     )\n\u001b[0;32m   1433\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[0;32m   1434\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[1;32mc:\\Users\\ashwi\\kunal_programming\\UdemySelfLearning\\krish-genai-course\\venv-genai\\lib\\site-packages\\langchain_huggingface\\llms\\huggingface_endpoint.py:258\u001b[0m, in \u001b[0;36mHuggingFaceEndpoint._call\u001b[1;34m(self, prompt, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    255\u001b[0m     invocation_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m invocation_params[\n\u001b[0;32m    256\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop_sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    257\u001b[0m     ]  \u001b[38;5;66;03m# porting 'stop_sequences' into the 'stop' argument\u001b[39;00m\n\u001b[1;32m--> 258\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparameters\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minvocation_params\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    263\u001b[0m     response_text \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(response\u001b[38;5;241m.\u001b[39mdecode())[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;66;03m# Maybe the generation has stopped at one of the stop sequences:\u001b[39;00m\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;66;03m# then we remove this stop sequence from the end of the generated text\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ashwi\\kunal_programming\\UdemySelfLearning\\krish-genai-course\\venv-genai\\lib\\site-packages\\huggingface_hub\\inference\\_client.py:304\u001b[0m, in \u001b[0;36mInferenceClient.post\u001b[1;34m(self, json, data, model, task, stream)\u001b[0m\n\u001b[0;32m    301\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceTimeoutError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference call timed out: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merror\u001b[39;00m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 304\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39miter_lines() \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[1;32mc:\\Users\\ashwi\\kunal_programming\\UdemySelfLearning\\krish-genai-course\\venv-genai\\lib\\site-packages\\huggingface_hub\\utils\\_errors.py:367\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m403\u001b[39m:\n\u001b[0;32m    361\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    362\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Forbidden: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCannot access content at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    364\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf you are trying to create or update content, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    365\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmake sure you have a token with the `write` role.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    366\u001b[0m     )\n\u001b[1;32m--> 367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HfHubHTTPError(message, response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HfHubHTTPError(\u001b[38;5;28mstr\u001b[39m(e), response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m:  (Request ID: J4weV2wAsUXoJZM0JHFcI)\n\n403 Forbidden: None.\nCannot access content at: https://api-inference.huggingface.co/models/google/gemma-2-9b.\nIf you are trying to create or update content, make sure you have a token with the `write` role.\nThe model google/gemma-2-9b is too large to be loaded automatically (36GB > 10GB). Please use Spaces (https://huggingface.co/spaces) or Inference Endpoints (https://huggingface.co/inference-endpoints)."
     ]
    }
   ],
   "source": [
    "llm.invoke(\"What is rate of interest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! max_length is not default parameter.\n",
      "                    max_length was transferred to model_kwargs.\n",
      "                    Please make sure that max_length is what you intended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HuggingFaceEndpoint(repo_id='mistralai/Mistral-7B-Instruct-v0.3', temperature=0.7, model_kwargs={'max_length': 150}, model='mistralai/Mistral-7B-Instruct-v0.3', client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.3', timeout=120)>, async_client=<InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.3', timeout=120)>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "llm=HuggingFaceEndpoint(repo_id=repo_id,max_length=150,temperature=0.7)\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'? The Cricket World Cup 2023 has not taken place yet. The next Cricket World Cup is scheduled to be held in India in 2023. The winner of the previous Cricket World Cup, which was held in 2019, was England.\\n\\nThe Cricket World Cup is an international cricket tournament that takes place every four years and is organized by the International Cricket Council (ICC). The tournament features the top international cricket teams and is considered to be the most prestigious event in the sport. The next Cricket World Cup is scheduled to be held in India in 2023, and all the cricket fans are eagerly waiting for the event.\\n\\nThe Cricket World Cup 2023 will be the 13th edition of the tournament, and it will be played in a round-robin format, with each team playing against every other team once. The top four teams from the round-robin stage will then advance to the semi-finals, and the winners of the semi-finals will play in the final to determine the champion.\\n\\nAs of now, it is not known who will win the Cricket World Cup 2023, as the tournament has not yet taken place. However, the cricket fans are already speculating about the possible winners, and some of the teams that are considered to be the favorites to win the tournament include India, England, Australia, and South Africa.\\n\\nIn conclusion, the Cricket World Cup 2023 has not taken place yet, and it is scheduled to be held in India in 2023. The winner of the previous Cricket World Cup was England, and some of the teams that are considered to be the favorites to win the tournament include India, England, Australia, and South Africa. The cricket fans are eagerly waiting for the event, and we will have to wait and see who will emerge as the champion of the Cricket World Cup 2023.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "prompt = \"\"\"\n",
    "You are a helpful assistant answer the following question using your knowledge.\n",
    "{question}\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(template=prompt, input_variables=['question'])\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "llm.invoke(\"Who won the cricket World Cup 2023\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ashwi\\kunal_programming\\UdemySelfLearning\\krish-genai-course\\venv-genai\\lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ashwi\\.cache\\huggingface\\hub\\models--BAAI--bge-small-en. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-small-en\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "hf = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = hf.embed_query(\"hi this Hari\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.04671413451433182,\n",
       " -0.005620173644274473,\n",
       " 0.015915436670184135,\n",
       " -0.029841849580407143,\n",
       " 0.018669644370675087,\n",
       " 0.0090032909065485,\n",
       " 0.07552587985992432,\n",
       " 0.029659178107976913,\n",
       " -0.018439948558807373,\n",
       " -0.004837920889258385,\n",
       " -0.006372713949531317,\n",
       " -0.08395711332559586,\n",
       " 0.03143174573779106,\n",
       " -0.0036078256089240313,\n",
       " 0.018534915521740913,\n",
       " -0.048435043543577194,\n",
       " 0.024245906621217728,\n",
       " -0.00018106139032170177,\n",
       " -0.03271406888961792,\n",
       " 0.019869482144713402,\n",
       " 0.008151753805577755,\n",
       " 0.004936059471219778,\n",
       " -0.044406693428754807,\n",
       " -0.015585193410515785,\n",
       " 0.03479338064789772,\n",
       " -0.009218728169798851,\n",
       " -0.026033597066998482,\n",
       " -0.026536287739872932,\n",
       " -0.024807196110486984,\n",
       " -0.18827693164348602,\n",
       " -0.006560035515576601,\n",
       " -0.019593987613916397,\n",
       " 0.06693793088197708,\n",
       " -0.013699018396437168,\n",
       " -0.012995012104511261,\n",
       " -0.0030323618557304144,\n",
       " 0.0023140525445342064,\n",
       " 0.03961927816271782,\n",
       " -0.03837090730667114,\n",
       " -0.006475402042269707,\n",
       " 0.029867425560951233,\n",
       " 0.006136724725365639,\n",
       " 0.02087493985891342,\n",
       " -0.004939334932714701,\n",
       " -0.010271682403981686,\n",
       " -0.03025795705616474,\n",
       " -0.02178780362010002,\n",
       " -0.013792899437248707,\n",
       " 0.03570191189646721,\n",
       " 0.0009043445461429656,\n",
       " -0.017181791365146637,\n",
       " -0.0005124162416905165,\n",
       " 0.010886194184422493,\n",
       " -0.01845184899866581,\n",
       " 0.0053443280048668385,\n",
       " 0.0248042494058609,\n",
       " 0.08373092114925385,\n",
       " 0.013411990366876125,\n",
       " 0.043745361268520355,\n",
       " 0.007963057607412338,\n",
       " 0.025485211983323097,\n",
       " 0.03208980709314346,\n",
       " -0.17157669365406036,\n",
       " 0.040544260293245316,\n",
       " -0.03424764424562454,\n",
       " -0.012821215204894543,\n",
       " -0.011181185021996498,\n",
       " -0.03584187477827072,\n",
       " 0.05663738399744034,\n",
       " 0.040586087852716446,\n",
       " -0.012016285210847855,\n",
       " -0.018011484295129776,\n",
       " -0.008756383322179317,\n",
       " 0.010953755117952824,\n",
       " 0.00039615880814380944,\n",
       " -0.02205585688352585,\n",
       " 0.021854689344763756,\n",
       " -0.01151162851601839,\n",
       " -0.0029228869825601578,\n",
       " -0.03717804327607155,\n",
       " -0.011671867221593857,\n",
       " -0.0015214032027870417,\n",
       " 0.009268644265830517,\n",
       " 5.201254680287093e-05,\n",
       " -0.03104300983250141,\n",
       " -0.03109823912382126,\n",
       " 0.03466135635972023,\n",
       " -0.009161088615655899,\n",
       " 0.015192057006061077,\n",
       " -0.0026282749604433775,\n",
       " -0.006427408196032047,\n",
       " -0.02265768311917782,\n",
       " -0.01357207726687193,\n",
       " -0.00477035716176033,\n",
       " -0.058435358107089996,\n",
       " -0.0026086224243044853,\n",
       " -0.01339848805218935,\n",
       " -0.012274133041501045,\n",
       " -0.05336497724056244,\n",
       " 0.5890305042266846,\n",
       " -0.04794645309448242,\n",
       " 0.00829942338168621,\n",
       " 0.012622094713151455,\n",
       " -0.05163147673010826,\n",
       " -0.017803311347961426,\n",
       " 0.021076129749417305,\n",
       " -0.0008730895933695138,\n",
       " 0.004684253130108118,\n",
       " -0.0012495263945311308,\n",
       " -0.007411116734147072,\n",
       " 0.007171896751970053,\n",
       " -0.031128719449043274,\n",
       " 0.05330318585038185,\n",
       " -0.007616644725203514,\n",
       " 0.011078228242695332,\n",
       " -0.020789407193660736,\n",
       " 0.04883599653840065,\n",
       " -0.0076230731792747974,\n",
       " -0.008561593480408192,\n",
       " -0.015539039857685566,\n",
       " -0.02387141063809395,\n",
       " 0.026169797405600548,\n",
       " 0.01923147775232792,\n",
       " -0.022965243086218834,\n",
       " 0.06903069466352463,\n",
       " -0.02989817224442959,\n",
       " 0.00698250625282526,\n",
       " 0.024047451093792915,\n",
       " 0.036819566041231155,\n",
       " -0.02020217664539814,\n",
       " 0.029288597404956818,\n",
       " -0.005225451197475195,\n",
       " -0.01766502857208252,\n",
       " -0.018651196733117104,\n",
       " -0.014067508280277252,\n",
       " 0.014287092722952366,\n",
       " 0.02246204949915409,\n",
       " -0.015624468214809895,\n",
       " -0.0024022117722779512,\n",
       " -0.02996346913278103,\n",
       " -0.04984124004840851,\n",
       " -0.07663922011852264,\n",
       " -0.047505877912044525,\n",
       " -0.0534343421459198,\n",
       " -0.0517515204846859,\n",
       " 0.013183084316551685,\n",
       " -0.03783108666539192,\n",
       " 0.0285231601446867,\n",
       " 0.0014575155219063163,\n",
       " -0.013294147327542305,\n",
       " -0.0055089970119297504,\n",
       " 0.017475279048085213,\n",
       " -0.033755991607904434,\n",
       " -0.029948847368359566,\n",
       " 0.03297242522239685,\n",
       " 0.0020099368412047625,\n",
       " 0.0893213078379631,\n",
       " 0.048119280487298965,\n",
       " -0.02976829744875431,\n",
       " -0.014681215398013592,\n",
       " -0.02233698219060898,\n",
       " -0.03936386480927467,\n",
       " -0.018642334267497063,\n",
       " 0.09379145503044128,\n",
       " 0.005996917840093374,\n",
       " -0.10153497755527496,\n",
       " -0.054486021399497986,\n",
       " -0.01677626371383667,\n",
       " 0.00950231309980154,\n",
       " -0.0023667931091040373,\n",
       " 0.050535086542367935,\n",
       " 0.02328675054013729,\n",
       " -0.02896198071539402,\n",
       " 0.03092741221189499,\n",
       " 0.08885611593723297,\n",
       " 0.005862668622285128,\n",
       " 0.017510373145341873,\n",
       " -0.0027732085436582565,\n",
       " 0.0035792116541415453,\n",
       " 0.0303067397326231,\n",
       " 0.04710158333182335,\n",
       " -0.010124516673386097,\n",
       " -0.04748956114053726,\n",
       " 0.008472030982375145,\n",
       " 0.017051728442311287,\n",
       " -0.03435083106160164,\n",
       " -0.028490567579865456,\n",
       " -0.0004908261471427977,\n",
       " -0.008129937574267387,\n",
       " 0.01052790880203247,\n",
       " -0.018479544669389725,\n",
       " 0.06882839649915695,\n",
       " -0.03191319853067398,\n",
       " 0.018020423129200935,\n",
       " 0.009660959243774414,\n",
       " -0.004880319815129042,\n",
       " -0.0040921298786997795,\n",
       " 0.0011893778573721647,\n",
       " 0.04603912681341171,\n",
       " 0.0019059462938457727,\n",
       " -0.03481700271368027,\n",
       " 0.019569989293813705,\n",
       " -0.008054669946432114,\n",
       " 0.042682431638240814,\n",
       " -0.003035480622202158,\n",
       " 0.05434019863605499,\n",
       " 0.011067885905504227,\n",
       " -0.04153798893094063,\n",
       " -0.01112421602010727,\n",
       " -0.0019324006279930472,\n",
       " -0.03400726616382599,\n",
       " 0.001648642704822123,\n",
       " 0.0068768830969929695,\n",
       " 0.010862653143703938,\n",
       " 0.004490047227591276,\n",
       " -0.010264416225254536,\n",
       " -0.0016515884781256318,\n",
       " 0.056706562638282776,\n",
       " -0.04471429064869881,\n",
       " 0.029809245839715004,\n",
       " 0.011429172940552235,\n",
       " -0.07157576829195023,\n",
       " -0.05607609450817108,\n",
       " -0.21667762100696564,\n",
       " 0.01892269030213356,\n",
       " 0.029755868017673492,\n",
       " -0.012414486147463322,\n",
       " 0.02917248010635376,\n",
       " -0.05921114608645439,\n",
       " 0.03846752271056175,\n",
       " -0.0046639651991426945,\n",
       " 0.08076522499322891,\n",
       " 0.00885146576911211,\n",
       " 0.06792756170034409,\n",
       " 0.00485004810616374,\n",
       " -0.006286875810474157,\n",
       " 0.023099053651094437,\n",
       " 0.011867743916809559,\n",
       " 0.052691858261823654,\n",
       " 0.01851538196206093,\n",
       " -0.002159396419301629,\n",
       " 0.02672770246863365,\n",
       " -0.04309548810124397,\n",
       " 0.08106181770563126,\n",
       " 0.019592680037021637,\n",
       " -0.01050569023936987,\n",
       " -0.056442778557538986,\n",
       " 0.0793086513876915,\n",
       " -0.011340704746544361,\n",
       " 0.1375369280576706,\n",
       " 0.07839043438434601,\n",
       " 0.04348619654774666,\n",
       " -0.03192584961652756,\n",
       " 0.05390350893139839,\n",
       " 0.013847626745700836,\n",
       " 0.01329706609249115,\n",
       " -0.13981947302818298,\n",
       " 0.021623125299811363,\n",
       " 0.018588146194815636,\n",
       " -0.009368589147925377,\n",
       " -0.08264616131782532,\n",
       " -0.06467216461896896,\n",
       " 0.013521424494683743,\n",
       " -0.03529009595513344,\n",
       " 0.0153552470728755,\n",
       " -0.02987709641456604,\n",
       " -0.04270024225115776,\n",
       " -0.04024992138147354,\n",
       " -0.05300426110625267,\n",
       " -0.025334497913718224,\n",
       " 0.02212069369852543,\n",
       " -0.024843275547027588,\n",
       " 0.06618958711624146,\n",
       " 0.014572720043361187,\n",
       " -0.05477312207221985,\n",
       " -0.020211802795529366,\n",
       " -0.0005658885929733515,\n",
       " -0.053388532251119614,\n",
       " -0.06295172870159149,\n",
       " -0.012981932610273361,\n",
       " -0.03266073763370514,\n",
       " -0.03219570964574814,\n",
       " 0.027383623644709587,\n",
       " 0.009743778966367245,\n",
       " 0.015235555358231068,\n",
       " 0.012524412013590336,\n",
       " 0.018745053559541702,\n",
       " -0.003734329715371132,\n",
       " 0.0189429372549057,\n",
       " 0.00175710569601506,\n",
       " 0.012463494203984737,\n",
       " -0.030285920947790146,\n",
       " -0.0037901264149695635,\n",
       " -0.029883775860071182,\n",
       " 0.05823434516787529,\n",
       " -0.04633598402142525,\n",
       " -0.009378890506923199,\n",
       " 0.037452954798936844,\n",
       " 0.03744960576295853,\n",
       " 0.025043915957212448,\n",
       " -0.03660581633448601,\n",
       " -0.046106934547424316,\n",
       " -0.004447519779205322,\n",
       " 0.0021424349397420883,\n",
       " 0.05248786509037018,\n",
       " 0.009028767235577106,\n",
       " 0.05494779348373413,\n",
       " -0.016912588849663734,\n",
       " 0.03848443552851677,\n",
       " 0.028796417638659477,\n",
       " -0.010721801780164242,\n",
       " 0.019944531843066216,\n",
       " -0.06808911263942719,\n",
       " 0.00972050242125988,\n",
       " 0.005662092473357916,\n",
       " 0.008534942753612995,\n",
       " -0.07108908146619797,\n",
       " -0.006950389128178358,\n",
       " -0.006592306774109602,\n",
       " -0.26204678416252136,\n",
       " 0.0445113405585289,\n",
       " -0.04811345040798187,\n",
       " 0.009799022227525711,\n",
       " -0.006440365687012672,\n",
       " -0.0077633485198020935,\n",
       " 0.0005440189852379262,\n",
       " -0.00730478297919035,\n",
       " -0.03567094728350639,\n",
       " 0.008928155526518822,\n",
       " 0.062489692121744156,\n",
       " 0.07171525061130524,\n",
       " 0.03465338796377182,\n",
       " 0.056557852774858475,\n",
       " 0.006519694346934557,\n",
       " -0.004970119800418615,\n",
       " 0.028632430359721184,\n",
       " 0.02910519577562809,\n",
       " -0.0022477598395198584,\n",
       " -0.02730540744960308,\n",
       " -0.011219707317650318,\n",
       " -0.006442111451178789,\n",
       " 0.12487880885601044,\n",
       " -0.007358322385698557,\n",
       " 0.018257781863212585,\n",
       " 0.014631040394306183,\n",
       " 0.010076845064759254,\n",
       " 0.02567841112613678,\n",
       " -0.06733804941177368,\n",
       " -0.03020482137799263,\n",
       " 0.00052775873336941,\n",
       " -0.0013440215261653066,\n",
       " 0.07301170378923416,\n",
       " -0.0182583536952734,\n",
       " 0.017492126673460007,\n",
       " 0.006913361605256796,\n",
       " -0.019503360614180565,\n",
       " 0.06396179646253586,\n",
       " 0.03791549429297447,\n",
       " 0.01711496338248253,\n",
       " 0.01949252374470234,\n",
       " 0.04118945822119713,\n",
       " -0.08357404172420502,\n",
       " -0.004561703186482191,\n",
       " 0.043776094913482666,\n",
       " -0.07636972516775131,\n",
       " 0.003823784412816167,\n",
       " -0.010607671923935413,\n",
       " -0.0134065430611372,\n",
       " 0.013701883144676685,\n",
       " 0.04028903320431709,\n",
       " -0.011408859863877296,\n",
       " 0.02301437221467495,\n",
       " 0.040438614785671234,\n",
       " -0.006288048345595598,\n",
       " -0.013762862421572208,\n",
       " -0.02150196023285389,\n",
       " -0.017887448891997337,\n",
       " -0.032555751502513885,\n",
       " -0.0036021112464368343,\n",
       " 0.019505027681589127,\n",
       " -0.01992206461727619,\n",
       " 0.02807018533349037,\n",
       " 0.07453378289937973,\n",
       " 0.027444321662187576]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
